{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ade49d",
   "metadata": {},
   "source": [
    "pose estimation(자세 추정) : 이미지나 영상 속 사람의 관절 위치(key point)를 추정\n",
    "\n",
    "→ 신체의 주요 관절 좌표 예측\n",
    "\n",
    "*활용 분야 : 자세 교정/환자 운동 범위 추적/재활 진행도 분석/특정 동작 인식/모션 캡쳐 대체 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons1.jpg: 640x640 4 persons, 164.4ms\n",
      "Speed: 8.7ms preprocess, 164.4ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")  # 가장 가벼운 pose 모델 로드\n",
    "results = model(\"persons1.jpg\", save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7159660",
   "metadata": {},
   "source": [
    "preprocess 이미지 리사이즈 및 정규화 등 사전 처리 시간 \n",
    "inference 모델이 이미지를 보고 예측한 시간\n",
    "postprocess 결과 필터링, NMS, 시각화용 후처리 시간\n",
    "\n",
    "*NMS(Non-Maximum Suppression) 객체 탐지에서 겹치는 박스를 정리해서 하나로 줄여주는 알고리즘 -> 객체 하나 당 하나의 박스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe62e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons2.jpg: 640x480 1 person, 157.8ms\n",
      "Speed: 5.0ms preprocess, 157.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\pose\\predict2\u001b[0m\n",
      "예측된 키포인트 좌표:\n",
      " tensor([[[   0.0000,    0.0000],\n",
      "         [   0.0000,    0.0000],\n",
      "         [   0.0000,    0.0000],\n",
      "         [   0.0000,    0.0000],\n",
      "         [ 532.5292,  941.6346],\n",
      "         [ 527.2236,  996.5807],\n",
      "         [ 523.4362,  995.3266],\n",
      "         [ 528.7708, 1073.4230],\n",
      "         [ 515.8889, 1068.6429],\n",
      "         [ 540.0588, 1140.7634],\n",
      "         [ 511.3009, 1140.2900],\n",
      "         [ 542.0768, 1143.8602],\n",
      "         [ 533.9288, 1144.5299],\n",
      "         [ 560.5419, 1252.8081],\n",
      "         [ 517.6130, 1255.1003],\n",
      "         [ 552.4837, 1349.8937],\n",
      "         [ 467.4814, 1355.7496]]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO        # 라이브러리 불러오기\n",
    "model = YOLO(\"yolov8n-pose.pt\")     # 모델 로드\n",
    "image_path = \"persons2.jpg\"         # 이미지 불러오기\n",
    "\n",
    "# 포즈 예측\n",
    "results = model(image_path, save=True)  # save=True는 이미지 저장됨 (runs 폴더에)\n",
    "\n",
    "# 키포인트 값 확인\n",
    "keypoints = results[0].keypoints.xy\n",
    "print(\"예측된 키포인트 좌표:\\n\", keypoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406c25f",
   "metadata": {},
   "source": [
    "shape(1,3,640,480)\n",
    "1장의 이미지\n",
    "3개의 채널(RGB 이미지)\n",
    "height x width\n",
    "-> yolov8은 보통 이미지를 모델에 입력 전 정사각형으로 리사이즈함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons3.jpg: 640x480 9 persons, 104.9ms\n",
      "Speed: 3.0ms preprocess, 104.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\pose\\predict3\u001b[0m\n",
      "사람 박스들의 confidence 값:\n",
      "tensor([0.8563, 0.7901, 0.7793, 0.6388, 0.6226, 0.6212, 0.4714, 0.4177, 0.2917])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "image_path = \"persons3.jpg\"\n",
    "\n",
    "# 포즈 예측\n",
    "results = model(image_path, save=True)\n",
    "\n",
    "# 각 탐지된 객체의 confidence 값\n",
    "confidences = results[0].boxes.conf  # tensor 형태 (여러 벡터를 한 행렬에 표기)\n",
    "print(\"사람 박스들의 confidence 값:\")\n",
    "print(confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44037e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons4.jpg: 640x640 5 persons, 132.9ms\n",
      "Speed: 3.8ms preprocess, 132.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\pose\\predict4\u001b[0m\n",
      "사람 박스들의 confidence 값:\n",
      "tensor([0.9204, 0.8834, 0.8687, 0.8653, 0.6199])\n",
      "예측된 키포인트 좌표:\n",
      " tensor([[[175.2481, 268.1964],\n",
      "         [  0.0000,   0.0000],\n",
      "         [169.0954, 262.6559],\n",
      "         [  0.0000,   0.0000],\n",
      "         [150.2312, 268.1085],\n",
      "         [160.6643, 309.5110],\n",
      "         [113.3800, 289.0453],\n",
      "         [173.8497, 348.3532],\n",
      "         [ 62.6228, 298.9847],\n",
      "         [202.3804, 350.5340],\n",
      "         [107.7710, 326.9831],\n",
      "         [146.5933, 374.1945],\n",
      "         [119.0605, 375.0044],\n",
      "         [192.5032, 372.6408],\n",
      "         [175.9930, 382.3507],\n",
      "         [198.6234, 442.7246],\n",
      "         [179.9709, 456.9413]],\n",
      "\n",
      "        [[325.4953, 242.8449],\n",
      "         [328.1057, 240.4042],\n",
      "         [321.7663, 240.4027],\n",
      "         [331.6931, 243.2339],\n",
      "         [315.9515, 243.8944],\n",
      "         [340.8648, 258.5007],\n",
      "         [306.3673, 267.2572],\n",
      "         [366.5128, 253.0805],\n",
      "         [294.3586, 290.8212],\n",
      "         [385.2062, 236.0264],\n",
      "         [297.1127, 313.2642],\n",
      "         [333.0750, 312.2595],\n",
      "         [312.7926, 315.5377],\n",
      "         [329.7449, 350.4996],\n",
      "         [313.7198, 353.6036],\n",
      "         [316.6105, 385.2708],\n",
      "         [307.5176, 389.0200]],\n",
      "\n",
      "        [[253.2999, 296.7496],\n",
      "         [255.6695, 294.0388],\n",
      "         [249.4846, 294.2289],\n",
      "         [  0.0000,   0.0000],\n",
      "         [241.7420, 297.0824],\n",
      "         [265.3463, 313.7014],\n",
      "         [230.8049, 314.3155],\n",
      "         [281.2909, 328.2516],\n",
      "         [219.0910, 332.0368],\n",
      "         [280.8613, 325.9000],\n",
      "         [224.8274, 335.6917],\n",
      "         [267.8566, 353.4078],\n",
      "         [246.8713, 355.9062],\n",
      "         [285.6793, 366.2425],\n",
      "         [271.5815, 367.6946],\n",
      "         [291.1830, 405.1805],\n",
      "         [287.6640, 406.7986]],\n",
      "\n",
      "        [[451.7801, 329.7853],\n",
      "         [  0.0000,   0.0000],\n",
      "         [449.1706, 325.0923],\n",
      "         [  0.0000,   0.0000],\n",
      "         [437.5528, 328.4703],\n",
      "         [418.1163, 352.6574],\n",
      "         [433.9591, 354.4450],\n",
      "         [444.0243, 383.7978],\n",
      "         [464.9405, 384.6679],\n",
      "         [483.7345, 399.0586],\n",
      "         [500.2353, 398.1123],\n",
      "         [417.9153, 418.2440],\n",
      "         [427.2088, 423.6819],\n",
      "         [485.4730, 395.6703],\n",
      "         [495.9344, 402.8611],\n",
      "         [473.9131, 429.0107],\n",
      "         [479.6156, 437.1776]],\n",
      "\n",
      "        [[ 42.8646, 423.5399],\n",
      "         [ 49.0628, 412.5207],\n",
      "         [ 29.9871, 412.2464],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  6.5560, 415.8526],\n",
      "         [ 62.3426, 470.4265],\n",
      "         [  0.0000, 471.1617],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 모델 로드\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# 이미지 불러오기\n",
    "image_path = \"persons4.jpg\"\n",
    "\n",
    "# 포즈 예측\n",
    "results = model(image_path, save=True)  # save=True는 이미지 저장됨 (runs 폴더에)\n",
    "\n",
    "# 각 탐지된 객체의 confidence 값\n",
    "confidences = results[0].boxes.conf  # tensor 형태\n",
    "print(\"사람 박스들의 confidence 값:\")\n",
    "print(confidences)\n",
    "\n",
    "# 키포인트 값 확인\n",
    "keypoints = results[0].keypoints.xy\n",
    "print(\"예측된 키포인트 좌표:\\n\", keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7be35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons6.jpg: 640x512 1 person, 75.3ms\n",
      "Speed: 2.3ms preprocess, 75.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns\\pose\\predict8\u001b[0m\n",
      "왼무릎-왼발목 거리: 175.08 픽셀\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "image_path = \"persons6.jpg\"\n",
    "results = model(image_path, save=True)\n",
    "\n",
    "# 첫 번째 사람의 keypoint 좌표 추출\n",
    "keypoints = results[0].keypoints.xy  # shape: (num_people, num_keypoints, 2(차원))\n",
    "\n",
    "if keypoints is not None and len(keypoints) > 0: # 예측된 사람이 1명 이상 있고, 키포인트가 있는지\n",
    "    person_kpts = keypoints[0]  # 첫 번째 사람\n",
    "\n",
    "    # 예: 다리(종아리) 길이 : 무릎-발목 거리\n",
    "    knee = person_kpts[13]\n",
    "    ankle = person_kpts[15]\n",
    "\n",
    "    # 좌표\n",
    "    x1, y1 = knee\n",
    "    x2, y2 = ankle\n",
    "\n",
    "    # 유클리드 거리 계산\n",
    "    distance = math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "    print(f\"왼무릎-왼발목 거리: {distance:.2f} 픽셀\")\n",
    "else:\n",
    "    print(\"사람이 감지되지 않았거나 keypoints가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95e5647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\yolo-pose-training\\persons6.jpg: 640x512 1 person, 71.0ms\n",
      "Speed: 3.5ms preprocess, 71.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns\\pose\\predict9\u001b[0m\n",
      "어깨 너비: 114.85 픽셀\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "image_path = \"persons6.jpg\"\n",
    "results = model(image_path, save=True)\n",
    "\n",
    "# 첫 번째 사람의 keypoint 좌표 추출\n",
    "keypoints = results[0].keypoints.xy  # shape: (num_people, num_keypoints, 2(차원))\n",
    "\n",
    "if keypoints is not None and len(keypoints) > 0: # 예측된 사람이 1명 이상 있고, 키포인트가 있는지\n",
    "    person_kpts = keypoints[0]  # 첫 번째 사람\n",
    "\n",
    "    # 어깨\n",
    "    left_sho = person_kpts[5]\n",
    "    right_sho = person_kpts[6]\n",
    "\n",
    "    # 좌표\n",
    "    x1, y1 = left_sho\n",
    "    x2, y2 = right_sho\n",
    "\n",
    "    # 유클리드 거리 계산\n",
    "    distance = math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "    print(f\"어깨 너비: {distance:.2f} 픽셀\")\n",
    "else:\n",
    "    print(\"사람이 감지되지 않았거나 keypoints가 없습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
